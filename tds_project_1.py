# -*- coding: utf-8 -*-
"""tds_project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EpL_E1-e5WjwqD-NXvc6TWkz5s0z1H50
"""

# Install required libraries (requests)
!pip install requests

# Import necessary libraries me
import requests
import csv
from google.colab import files

# Use Colab's input() function to securely input your GitHub Personal Access Token (PAT)
GITHUB_TOKEN = input("Please enter your GitHub Personal Access Token: ")
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# Function to fetch GitHub users from Chicago with over 100 followers
def fetch_users_in_chicago():
    users = []
    url = "https://api.github.com/search/users?q=location:chicago+followers:>100&per_page=100"

    while url:
        response = requests.get(url, headers=headers)
        response_json = response.json()
        users.extend(response_json['items'])  # Add users to the list

        # Handling pagination using the 'Link' header
        if 'next' in response.links:
            url = response.links['next']['url']
        else:
            url = None

    return users

# Function to fetch detailed user information
def fetch_user_details(username):
    url = f"https://api.github.com/users/{username}"
    response = requests.get(url, headers=headers)
    return response.json()

# Function to fetch repositories for a user (up to 500 repositories)
def fetch_user_repos(username):
    repos = []
    url = f"https://api.github.com/users/{username}/repos?per_page=100"

    while url:
        response = requests.get(url, headers=headers)
        repos.extend(response.json())  # Add repositories to the list

        # Handling pagination using the 'Link' header
        if 'next' in response.links:
            url = response.links['next']['url']
        else:
            url = None

    return repos

# Helper function to clean company names
def clean_company_name(company):
    if company:
        return company.strip().lstrip('@').upper()  # Remove @ and extra spaces, convert to uppercase
    return ''

# Write users data to CSV
def write_users_csv(users):
    with open('users.csv', mode='w', newline='') as file:
        writer = csv.writer(file)
        # Writing the headers
        writer.writerow(["login", "name", "company", "location", "email", "hireable", "bio", "public_repos", "followers", "following", "created_at"])

        for user in users:
            writer.writerow([
                user['login'],
                user['name'],
                clean_company_name(user.get('company', '')),
                user['location'],
                user.get('email', ''),
                user.get('hireable', ''),
                user.get('bio', ''),
                user['public_repos'],
                user['followers'],
                user['following'],
                user['created_at']
            ])

# Write repositories data to CSV
def write_repos_csv(repos):
    with open('repositories.csv', mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["login", "full_name", "created_at", "stargazers_count", "watchers_count", "language", "has_projects", "has_wiki", "license_name"])

        for repo in repos:
            writer.writerow([
                repo['owner']['login'],
                repo['full_name'],
                repo['created_at'],
                repo['stargazers_count'],
                repo['watchers_count'],
                repo['language'],
                repo['has_projects'],
                repo['has_wiki'],
                repo['license']['key'] if repo.get('license') else 'No License'
            ])

# Main function to orchestrate the process
def main():
    # Fetch users in Chicago with more than 100 followers
    users = fetch_users_in_chicago()
    print("1 Fetching users function complete")

    # Fetch detailed information for each user
    detailed_users = [fetch_user_details(user['login']) for user in users]
    print("2 fetch_user_details complete")

    # Fetch repositories for each user
    repos = []
    for user in detailed_users:
        user_repos = fetch_user_repos(user['login'])
        repos.extend(user_repos)  # Collect all repositories
    print("3 fetch_user_repos complete")

    # Write user and repository data to CSV files
    write_users_csv(detailed_users)
    print("4 write_users_csv complete")

    write_repos_csv(repos)
    print("5 write_repos_csv complete")

print("Data scraping complete! CSV files generated.")

# Execute the main function
main()

# Download the CSV files to your local machine
files.download('users.csv')
files.download('repositories.csv')

from google.colab import files
uploaded = files.upload()

# (Q1) Who are the top 5 users in Chicago with the highest number of followers? List their login in order, comma-separated.
# Import necessary libraries
import pandas as pd

# Read the CSV files
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary
# Display the first few rows of the dataframe to verify data
print(users_df.head())

# Function to get the top 5 users with the highest number of followers
def top_5_users_by_followers(users_df):
    # Sort users by the number of followers in descending order
    sorted_users = users_df.sort_values(by='followers', ascending=False)

    # Extract the top 5 users
    top_5 = sorted_users.head(5)

    # Return their login names, comma-separated
    top_5_logins = ','.join(top_5['login'].tolist())

    return top_5_logins

# Get the top 5 users by followers
top_5_logins = top_5_users_by_followers(users_df)
print("Top 5 users by followers: ", top_5_logins)

# (Q2) Who are the 5 earliest registered GitHub users in Chicago? List their login in ascending order of created_at, comma-separated.
# Function to get the 5 earliest registered users
def earliest_5_users(users_df):
    # Convert the 'created_at' column to datetime
    users_df['created_at'] = pd.to_datetime(users_df['created_at'])

    # Sort users by 'created_at' in ascending order
    sorted_users = users_df.sort_values(by='created_at', ascending=True)

    # Extract the top 5 earliest users
    earliest_5 = sorted_users.head(5)

    # Return their login names, comma-separated
    earliest_5_logins = ','.join(earliest_5['login'].tolist())

    return earliest_5_logins

# Get the 5 earliest registered users
earliest_5_logins = earliest_5_users(users_df)
print("5 earliest registered users: ", earliest_5_logins)

# (Q3) Function to get the 3 most popular licenses
# Read the repositories CSV file
repos_df = pd.read_csv('/content/repositories.csv')  # Adjust the path if necessary

# Function to get the 3 most popular licenses
def top_3_popular_licenses(repos_df):
    # Filter out missing licenses
    filtered_licenses = repos_df['license_name'].dropna()

    # Count the occurrences of each license
    license_counts = filtered_licenses.value_counts()

    # Get the top 3 licenses
    top_3_licenses = license_counts.head(3)

    # Return the license names, comma-separated
    return ','.join(top_3_licenses.index.tolist())

# Get the 3 most popular licenses
top_3_licenses = top_3_popular_licenses(repos_df)
print("3 most popular licenses: ", top_3_licenses)

# (Q4) Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Function to get the majority company
def majority_company(users_df):
    # Filter out missing company names
    filtered_companies = users_df['company'].dropna()

    # Count the occurrences of each company
    company_counts = filtered_companies.value_counts()

    # Get the company with the highest count
    majority_company = company_counts.idxmax()  # Company name with the highest count
    majority_count = company_counts.max()  # Count of developers in that company

    return majority_company, majority_count

# Get the majority company and its count
company, count = majority_company(users_df)
print(f"The majority of developers work at: {company} (Count: {count})")

# (Q5)
# Read the repositories CSV file
repos_df = pd.read_csv('/content/repositories.csv')  # Adjust the path if necessary

# Function to get the most popular programming language
def most_popular_language(repos_df):
    # Filter out missing languages
    filtered_languages = repos_df['language'].dropna()

    # Count the occurrences of each language
    language_counts = filtered_languages.value_counts()

    # Get the most popular language
    most_popular_language = language_counts.idxmax()  # Language name with the highest count
    most_popular_count = language_counts.max()  # Count of repositories in that language

    return most_popular_language, most_popular_count

# Get the most popular language and its count
language, count = most_popular_language(repos_df)
print(f"The most popular programming language is: {language} (Count: {count})")

# (Q6)
# Read the users and repositories CSV files
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary
repos_df = pd.read_csv('/content/repositories.csv')  # Adjust the path if necessary

# Function to get the second most popular language among users who joined after 2020
def second_most_popular_language(users_df, repos_df):
    # Convert 'created_at' to datetime
    users_df['created_at'] = pd.to_datetime(users_df['created_at'])

    # Filter users who joined after 2020
    filtered_users = users_df[users_df['created_at'] > '2020-01-01']

    # Get the logins of these users
    logins_after_2020 = filtered_users['login'].tolist()

    # Filter repositories by these users
    filtered_repos = repos_df[repos_df['login'].isin(logins_after_2020)]

    # Filter out missing languages
    filtered_languages = filtered_repos['language'].dropna()

    # Count the occurrences of each language
    language_counts = filtered_languages.value_counts()

    # Get the second most popular language
    if len(language_counts) > 1:
        second_most_popular_language = language_counts.index[1]  # Get the second most popular language
        second_most_count = language_counts.iloc[1]  # Count of repositories in that language
    else:
        second_most_popular_language = None
        second_most_count = 0

    return second_most_popular_language, second_most_count

# Get the second most popular language and its count
language, count = second_most_popular_language(users_df, repos_df)
if language:
    print(f"The second most popular programming language among users who joined after 2020 is: {language} (Count: {count})")
else:
    print("Not enough data to determine the second most popular language.")

# (Q7)
# Read the repositories CSV file
repos_df = pd.read_csv('/content/repositories.csv')  # Adjust the path if necessary

# Function to find the language with the highest average number of stars per repository
def language_with_highest_avg_stars(repos_df):
    # Filter out missing languages and stars
    filtered_repos = repos_df[['language', 'stargazers_count']].dropna()

    # Group by language and calculate the average number of stars
    avg_stars = filtered_repos.groupby('language')['stargazers_count'].mean()

    # Get the language with the highest average stars
    highest_avg_language = avg_stars.idxmax()  # Language with the highest average stars
    highest_avg_count = avg_stars.max()  # Average number of stars for that language

    return highest_avg_language, highest_avg_count

# Get the language with the highest average stars
language, avg_stars = language_with_highest_avg_stars(repos_df)
print(f"The programming language with the highest average number of stars per repository is: {language} (Average Stars: {avg_stars:.2f})")

# (Q8)
# Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Calculate leader_strength
users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])

# Get the top 5 users by leader_strength
top_leaders = users_df.nlargest(5, 'leader_strength')[['login', 'leader_strength']]

# Prepare the output as a comma-separated string
top_leader_logins = ','.join(top_leaders['login'].tolist())

print(f"The top 5 users in terms of leader_strength are: {top_leader_logins}")

# (Q9)
# Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Calculate the correlation between followers and public repositories
correlation = users_df['followers'].corr(users_df['public_repos'])

# Print the correlation rounded to 3 decimal places
print(f"The correlation between the number of followers and the number of public repositories is: {correlation:.3f}")

# (Q10)
# Import the required library for linear regression
from scipy import stats

# Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Perform linear regression to find the slope
slope, intercept, r_value, p_value, std_err = stats.linregress(users_df['public_repos'], users_df['followers'])

# Print the slope rounded to 3 decimal places
print(f"The regression slope of followers on public repositories is: {slope:.3f}")

# (Q11)
# Import necessary libraries
import pandas as pd

# Read the repositories CSV file
repos_df = pd.read_csv('/content/repositories.csv')  # Adjust the path if necessary

# Convert has_projects and has_wiki to binary (1 for True, 0 for False)
repos_df['has_projects'] = repos_df['has_projects'].replace({True: 1, False: 0})
repos_df['has_wiki'] = repos_df['has_wiki'].replace({True: 1, False: 0})

# Calculate the correlation between has_projects and has_wiki
correlation = repos_df['has_projects'].corr(repos_df['has_wiki'])

# Print the correlation rounded to 3 decimal places
print(f"The correlation between having projects enabled and having a wiki enabled is: {correlation:.3f}")

# (Q12)
# Import necessary libraries
import pandas as pd

# Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Calculate average following for hireable users (hireable == True)
avg_following_hireable = users_df[users_df['hireable'] == True]['following'].mean()

# Calculate average following for non-hireable users (hireable == False)
avg_following_non_hireable = users_df[users_df['hireable'] == False]['following'].mean()

# Calculate the difference between the two averages
difference = avg_following_hireable - avg_following_non_hireable

# Print the result rounded to 3 decimal places
print(f"The difference in average following between hireable and non-hireable users is: {difference:.3f}")

# (Q13)
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Filter out users without bios
bio_users_df = users_df[users_df['bio'].notnull()]

# Calculate bio length in words
bio_users_df['bio_length'] = bio_users_df['bio'].apply(lambda x: len(x.split()))

# Prepare the data for regression
X = bio_users_df[['bio_length']].values  # Independent variable (bio length)
y = bio_users_df['followers'].values  # Dependent variable (followers)

# Create and fit the linear regression model
model = LinearRegression()
model.fit(X, y)

# Get the slope of the regression line (coefficient)
slope = model.coef_[0]

# Print the slope rounded to 3 decimal places
print(f"The regression slope of followers on bio word count is: {slope:.3f}")

# (Q14)
# Import necessary libraries
import pandas as pd

# Read the repositories CSV file
repos_df = pd.read_csv('/content/repositories.csv')  # Adjust the path if necessary

# Convert created_at to datetime
repos_df['created_at'] = pd.to_datetime(repos_df['created_at'])

# Filter for weekends (Saturday and Sunday)
repos_df['is_weekend'] = repos_df['created_at'].dt.dayofweek >= 5  # 5: Saturday, 6: Sunday

# Count the number of repositories created on weekends by each user
weekend_repos_count = repos_df[repos_df['is_weekend']].groupby('login').size()

# Get the top 5 users with the most weekend repositories
top_weekend_users = weekend_repos_count.nlargest(5)

# Get the logins of the top users
top_weekend_logins = top_weekend_users.index.tolist()

# Print the top 5 users' logins in order, comma-separated
print("Top 5 users who created the most repositories on weekends (UTC):", ','.join(top_weekend_logins))

# (Q15)
# Import necessary libraries
import pandas as pd

# Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Calculate fraction of users with email when hireable=true
hireable_users = users_df[users_df['hireable'] == True]
fraction_hireable_with_email = hireable_users['email'].notnull().mean()

# Calculate fraction of users with email when hireable=false
non_hireable_users = users_df[users_df['hireable'] == False]
fraction_non_hireable_with_email = non_hireable_users['email'].notnull().mean()

# Calculate the difference
difference = fraction_hireable_with_email - fraction_non_hireable_with_email

# Print the result rounded to 3 decimal places
print(f"Difference in fractions of users with email: {difference:.3f}")

# Q(16)
# Import necessary libraries
import pandas as pd

# Read the users CSV file
users_df = pd.read_csv('/content/users.csv')  # Adjust the path if necessary

# Extract surnames (last word in the name, ignoring missing names)
# Strip whitespace, dropna(), and then split by space to get the last element
surnames = users_df['name'].dropna().str.strip().str.split().str[-1]

# Count occurrences of each surname
surname_counts = surnames.value_counts()

# Find the maximum count of surnames
max_count = surname_counts.max()

# Get the most common surname(s)
most_common_surnames = surname_counts[surname_counts == max_count].index.tolist()

# Sort the surnames alphabetically
most_common_surnames.sort()

# Print the results
print(f"Most common surname(s): {','.join(most_common_surnames)}")
print(f"Number of users with the most common surname: {max_count}")